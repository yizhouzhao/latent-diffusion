{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e40d94d-dc95-46df-a77a-c9f8af63d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eca6874-345f-4cbc-bba9-fe3dfd0c5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "\n",
    "# add cwd for convenience and to make classes in this file available when\n",
    "# running as `python main.py`\n",
    "# (in particular `main.DataModuleFromConfig`)\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22478575-7779-42a6-983c-44460f11ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "parser = Trainer.add_argparse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927b79d6-c61d-4f55-acc5-f2605712530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5c3a2c-eae2-4fdb-aed1-02efc5f228f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, base=[], benchmark=None, check_val_every_n_epoch=1, checkpoint_callback=None, debug=False, default_root_dir=None, detect_anomaly=False, deterministic=False, devices=None, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, fast_dev_run=False, flush_logs_every_n_steps=None, gpus=None, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, log_gpu_memory=None, logdir='logs', logger=True, max_epochs=None, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', name='', no_test=False, num_nodes=1, num_processes=None, num_sanity_val_steps=2, overfit_batches=0.0, plugins=None, postfix='C:\\\\Users\\\\Yizhou Zhao\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-a384b2a0-0770-4fd0-832a-cc0d880e2b38.json', precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, project=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume='', resume_from_checkpoint=None, scale_lr=True, seed=23, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, terminate_on_nan=None, tpu_cores=None, track_grad_norm=-1, train=False, val_check_interval=None, weights_save_path=None, weights_summary='top')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03561e55-77e1-4d73-ba0e-29fd0a8b040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.base = ['configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml']\n",
    "opt.gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3242b7-4eb4-408b-ab06-9dd393295f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.postfix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d19dcc2e-79b8-4d10-8320-403de313fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if opt.name:\n",
    "    name = \"_\" + opt.name\n",
    "elif opt.base:\n",
    "    cfg_fname = os.path.split(opt.base[0])[-1]\n",
    "    cfg_name = os.path.splitext(cfg_fname)[0]\n",
    "    name = \"_\" + cfg_name\n",
    "else:\n",
    "    name = \"\"\n",
    "nowname = now + name + opt.postfix\n",
    "logdir = os.path.join(opt.logdir, nowname)\n",
    "\n",
    "ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "cfgdir = os.path.join(logdir, \"configs\")\n",
    "seed_everything(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5176fce-5d2e-4ad8-87e6-c207d32101e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-17T14-39-19_lsun_churches-ldm-kl-8\n"
     ]
    }
   ],
   "source": [
    "print(nowname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7f9fe9-7bbc-4cac-be95-b782b294a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c04e69a-8f66-43b8-8032-db956c91d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and save configs\n",
    "configs = [OmegaConf.load(cfg) for cfg in opt.base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdd8797-4a07-4f40-ab1e-f7b56ea180e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OmegaConf.from_dotlist(unknown)\n",
    "config = OmegaConf.merge(*configs, cli)\n",
    "lightning_config = config.pop(\"lightning\", OmegaConf.create())\n",
    "# merge trainer cli with config\n",
    "trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72656f7-5b99-43dd-9ee5-5acb734288ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPUs 1\n"
     ]
    }
   ],
   "source": [
    "# default to ddp\n",
    "trainer_config[\"accelerator\"] = \"gpu\"\n",
    "trainer_config[\"devices\"] = 1\n",
    "\n",
    "for k in nondefault_trainer_args(opt):\n",
    "    trainer_config[k] = getattr(opt, k)\n",
    "if not \"gpus\" in trainer_config:\n",
    "    del trainer_config[\"accelerator\"]\n",
    "    cpu = True\n",
    "else:\n",
    "    gpuinfo = trainer_config[\"gpus\"]\n",
    "    print(f\"Running on GPUs {gpuinfo}\")\n",
    "    cpu = False\n",
    "    \n",
    "trainer_opt = argparse.Namespace(**trainer_config)\n",
    "lightning_config.trainer = trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcedd649-c8f2-4431-b060-6e7053067caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model['params']['timesteps'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35e1251-97fd-4712-8773-0041cd024ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 294.97 M params.\n",
      "Keeping EMAs of 522.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Restored from models/first_stage_models/kl-f8/model.ckpt\n",
      "Training LatentDiffusion as an unconditional model.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = instantiate_from_config(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb59c589-526b-4532-8c25-65287c068196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark': True, 'accelerator': 'gpu', 'devices': 1, 'gpus': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c484eb5-0eaa-44be-9251-358e7470e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b3bc67a-6c0d-4427-973e-1d6895f044f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1082d522-1b63-414d-a6c0-ceedc9d4e4a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pause' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpause\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pause' is not defined"
     ]
    }
   ],
   "source": [
    "pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7da5967-f8ce-4e53-ab51-919075810398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0015, 0.0019, 0.0023, 0.0027, 0.0032, 0.0038, 0.0043, 0.0049, 0.0056,\n",
       "        0.0063, 0.0070, 0.0078, 0.0086, 0.0095, 0.0104, 0.0113, 0.0123, 0.0133,\n",
       "        0.0144, 0.0155])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc572b3b-472a-4632-933c-00ee21e3ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7495c76f-534e-4d25-b979-ff2f09827269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer and callbacks\n",
    "trainer_kwargs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8132d4e-93ea-435a-a0c5-64a58cc1842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring val/loss_simple_ema as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs\\\\2022-06-17T14-39-19_lsun_churches-ldm-kl-8\\\\checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\vrkit\\lib\\site-packages\\pytorch_lightning\\loggers\\test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "# default logger configs\n",
    "default_logger_cfgs = {\n",
    "    \"wandb\": {\n",
    "        \"target\": \"pytorch_lightning.loggers.WandbLogger\",\n",
    "        \"params\": {\n",
    "            \"name\": nowname,\n",
    "            \"save_dir\": logdir,\n",
    "            \"offline\": opt.debug,\n",
    "            \"id\": nowname,\n",
    "        }\n",
    "    },\n",
    "    \"testtube\": {\n",
    "        \"target\": \"pytorch_lightning.loggers.TestTubeLogger\",\n",
    "        \"params\": {\n",
    "            \"name\": \"testtube\",\n",
    "            \"save_dir\": logdir,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "default_logger_cfg = default_logger_cfgs[\"testtube\"]\n",
    "if \"logger\" in lightning_config:\n",
    "    logger_cfg = lightning_config.logger\n",
    "else:\n",
    "    logger_cfg = OmegaConf.create()\n",
    "logger_cfg = OmegaConf.merge(default_logger_cfg, logger_cfg)\n",
    "trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n",
    "\n",
    "# modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to\n",
    "# specify which metric is used to determine best models\n",
    "default_modelckpt_cfg = {\n",
    "    \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
    "    \"params\": {\n",
    "        \"dirpath\": ckptdir,\n",
    "        \"filename\": \"{epoch:06}\",\n",
    "        \"verbose\": True,\n",
    "        \"save_last\": True,\n",
    "    }\n",
    "}\n",
    "if hasattr(model, \"monitor\"):\n",
    "    print(f\"Monitoring {model.monitor} as checkpoint metric.\")\n",
    "    default_modelckpt_cfg[\"params\"][\"monitor\"] = model.monitor\n",
    "    default_modelckpt_cfg[\"params\"][\"save_top_k\"] = 3\n",
    "\n",
    "if \"modelcheckpoint\" in lightning_config:\n",
    "    modelckpt_cfg = lightning_config.modelcheckpoint\n",
    "else:\n",
    "    modelckpt_cfg =  OmegaConf.create()\n",
    "modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n",
    "print(f\"Merged modelckpt-cfg: \\n{modelckpt_cfg}\")\n",
    "if version.parse(pl.__version__) < version.parse('1.4.0'):\n",
    "    trainer_kwargs[\"checkpoint_callback\"] = instantiate_from_config(modelckpt_cfg)\n",
    "\n",
    "# add callback which sets up log directory\n",
    "default_callbacks_cfg = {\n",
    "    \"setup_callback\": {\n",
    "        \"target\": \"main.SetupCallback\",\n",
    "        \"params\": {\n",
    "            \"resume\": opt.resume,\n",
    "            \"now\": now,\n",
    "            \"logdir\": logdir,\n",
    "            \"ckptdir\": ckptdir,\n",
    "            \"cfgdir\": cfgdir,\n",
    "            \"config\": config,\n",
    "            \"lightning_config\": lightning_config,\n",
    "        }\n",
    "    },\n",
    "    \"image_logger\": {\n",
    "        \"target\": \"main.ImageLogger\",\n",
    "        \"params\": {\n",
    "            \"batch_frequency\": 750,\n",
    "            \"max_images\": 4,\n",
    "            \"clamp\": True\n",
    "        }\n",
    "    },\n",
    "    \"learning_rate_logger\": {\n",
    "        \"target\": \"main.LearningRateMonitor\",\n",
    "        \"params\": {\n",
    "            \"logging_interval\": \"step\",\n",
    "            # \"log_momentum\": True\n",
    "        }\n",
    "    },\n",
    "    \"cuda_callback\": {\n",
    "        \"target\": \"main.CUDACallback\"\n",
    "    },\n",
    "}\n",
    "if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
    "    default_callbacks_cfg.update({'checkpoint_callback': modelckpt_cfg})\n",
    "\n",
    "if \"callbacks\" in lightning_config:\n",
    "    callbacks_cfg = lightning_config.callbacks\n",
    "else:\n",
    "    callbacks_cfg = OmegaConf.create()\n",
    "\n",
    "if 'metrics_over_trainsteps_checkpoint' in callbacks_cfg:\n",
    "    print(\n",
    "        'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.')\n",
    "    default_metrics_over_trainsteps_ckpt_dict = {\n",
    "        'metrics_over_trainsteps_checkpoint':\n",
    "            {\"target\": 'pytorch_lightning.callbacks.ModelCheckpoint',\n",
    "             'params': {\n",
    "                 \"dirpath\": os.path.join(ckptdir, 'trainstep_checkpoints'),\n",
    "                 \"filename\": \"{epoch:06}-{step:09}\",\n",
    "                 \"verbose\": True,\n",
    "                 'save_top_k': -1,\n",
    "                 'every_n_train_steps': 10000,\n",
    "                 'save_weights_only': True\n",
    "             }\n",
    "             }\n",
    "    }\n",
    "    default_callbacks_cfg.update(default_metrics_over_trainsteps_ckpt_dict)\n",
    "\n",
    "callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n",
    "if 'ignore_keys_callback' in callbacks_cfg and hasattr(trainer_opt, 'resume_from_checkpoint'):\n",
    "    callbacks_cfg.ignore_keys_callback.params['ckpt_path'] = trainer_opt.resume_from_checkpoint\n",
    "elif 'ignore_keys_callback' in callbacks_cfg:\n",
    "    del callbacks_cfg['ignore_keys_callback']\n",
    "\n",
    "trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "620dfdb5-8418-4b5e-962b-6f61c1f47071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accelerator='gpu', benchmark=True, devices=1, gpus=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f77b321-da46-4349-9ec5-146997617145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\vrkit\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(accelerator=\"gpu\") # Trainer.from_argparse_args(trainer_opt, **trainer_kwargs)\n",
    "trainer.logdir = logdir  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7e0d730-00ca-45d5-ae8d-79b92dadd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data = instantiate_from_config(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52bf8ed9-8776-4303-a151-fb26e8861cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 24, 'num_workers': 5, 'wrap': False, 'train': {'target': 'ldm.data.lsun.LSUNChurchesTrain', 'params': {'size': 256}}, 'validation': {'target': 'ldm.data.lsun.LSUNChurchesValidation', 'params': {'size': 256}}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85029e0e-ebf1-4f3f-8419-1602de5d9cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<main.DataModuleFromConfig at 0x291520912e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15c82610-0495-47b0-8397-83ee2758e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "736a8b85-59ff-4f9a-8915-fb986f643395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dd21714-81ad-48a7-b594-3496ceb73a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, LSUNChurchesTrain, 121227\n",
      "validation, LSUNChurchesValidation, 5000\n"
     ]
    }
   ],
   "source": [
    "for k in data.datasets:\n",
    "    print(f\"{k}, {data.datasets[k].__class__.__name__}, {len(data.datasets[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75c447c5-f3b4-431c-905b-f7b6dee34ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulate_grad_batches = 1\n",
      "Setting learning rate to 1.20e-03 = 1 (accumulate_grad_batches) * 1 (num_gpus) * 24 (batchsize) * 5.00e-05 (base_lr)\n"
     ]
    }
   ],
   "source": [
    "# configure learning rate\n",
    "bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\n",
    "if not cpu:\n",
    "#     ngpu = len(lightning_config.trainer.gpus.strip(\",\").split(','))\n",
    "# else:\n",
    "    ngpu = 1\n",
    "if 'accumulate_grad_batches' in lightning_config.trainer:\n",
    "    accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n",
    "else:\n",
    "    accumulate_grad_batches = 1\n",
    "print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n",
    "if opt.scale_lr:\n",
    "    model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "    print(\n",
    "        \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "            model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "else:\n",
    "    model.learning_rate = base_lr\n",
    "    print(\"++++ NOT USING LR SCALING ++++\")\n",
    "    print(f\"Setting learning rate to {model.learning_rate:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12211f3e-f388-43f7-a3ee-49e0dfbda47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # allow checkpointing via USR1\n",
    "# def melk(*args, **kwargs):\n",
    "#     # run all checkpoint hooks\n",
    "#     if trainer.global_rank == 0:\n",
    "#         print(\"Summoning checkpoint.\")\n",
    "#         ckpt_path = os.path.join(ckptdir, \"last.ckpt\")\n",
    "#         trainer.save_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "# def divein(*args, **kwargs):\n",
    "#     if trainer.global_rank == 0:\n",
    "#         import pudb;\n",
    "#         pudb.set_trace()\n",
    "\n",
    "\n",
    "# import signal\n",
    "\n",
    "# signal.signal(signal.SIGUSR1, melk)\n",
    "# signal.signal(signal.SIGUSR2, divein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a76e6c26-dee9-4be1-a628-93a877dbd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b9bbf0-5fab-42bb-b051-fa0fa44e2696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs\\\\2022-06-17T14-39-19_lsun_churches-ldm-kl-8'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logdir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362be1d2-ced5-43f0-9e83-20ece128cbdb",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e16f4-9e34-4d08-93bf-9a17fa704258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\vrkit\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "Missing logger folder: D:\\research\\latent-diffusion\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper | 294 M \n",
      "1 | model_ema         | LitEma           | 0     \n",
      "2 | first_stage_model | AutoencoderKL    | 83.7 M\n",
      "-------------------------------------------------------\n",
      "294 M     Trainable params\n",
      "83.7 M    Non-trainable params\n",
      "378 M     Total params\n",
      "1,514.483 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LambdaLR scheduler...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bba810de8d44a7c8862700190c319f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### USING STD-RESCALING ###\n",
      "setting self.scale_factor to 0.21411871910095215\n",
      "### USING STD-RESCALING ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\envs\\vrkit\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:229: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62444179-fb8d-4cd5-a790-2fc0cb9bad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = data.train_dataloader()\n",
    "\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# batch['image'].shape\n",
    "\n",
    "# model.device\n",
    "\n",
    "# model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd31cc0-69ac-41c6-a96f-01739088f8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrkit",
   "language": "python",
   "name": "vrkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
